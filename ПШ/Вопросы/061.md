---
Приоритет:
  - ГЛАВНЫЙ
Direction:
  - "[[ПШ]]"
Discipline: Системы искусственного интеллекта
tags:
  - готово
---
[[060.md|<< Предыдущий вопрос]] | [[062.md|Следующий вопрос >>]]
## Вопрос
Модели искусственных нейронов. Модель МакКаллока-Питса. Персептрон. Правило персептрона.

### Что возможно нужно будет рассказать?
Вопрос направлен на понимание базовых моделей искусственных нейронов, которые лежат в основе нейронных сетей. Особое внимание уделяется историческим и концептуальным основам: модели МакКаллока-Питса — одной из первых формальных моделей нейрона, и персептрону — простейшей обучаемой модели нейрона.
#### **Подводные камни:**
- Нужно чётко разграничивать модель МакКаллока-Питса (логическая модель, «бинарный» нейрон) и персептрон (обучаемая модель с весами).
- Объяснить структуру каждой модели, функции активации и принцип работы.
- Правило персептрона — алгоритм обучения, который важно подробно расписать, включая математическую формулу обновления весов.
- Возможна необходимость упомянуть ограничения персептрона (например, невозможность решать задачи, неразделимые линейно).
- Хорошо привести пошаговый пример работы алгоритма обучения на простой задаче.

---
## Ответ
### 2.0 Модели искусственных нейронов
Искусственный нейрон — упрощённая математическая модель биологического нейрона, служащая элементом нейронных сетей. Его задача — принимать набор входных сигналов, обработать их и выдать выходной сигнал.

- **Модель МакКаллока-Питса**
    - Бинарный нейрон с пороговой функцией активации.
    - Основа для логических операций.
- **Персептрон**
    - Развитие МакКаллока-Питса с обучаемыми весами и пороговой активацией.
    - Подходит для задач линейной классификации.
- **Модель с непрерывной (гладкой) функцией активации**
    - Вместо пороговой используется сигмоидальная, гиперболический тангенс (tanh) или ReLU.
    - Позволяет работать с непрерывными выходами и использовать градиентные методы обучения (например, обратное распространение ошибки).
    - Пример: многослойный персептрон (MLP).
- **Радиально-базисная функция (RBF) нейрон**
    - Выход зависит от расстояния входа до центра (радиально-симметричная функция).
    - Используется для аппроксимации и кластеризации.
- **Пуассоновский нейрон** (используется в моделях биологических нейронов с учётом случайных импульсов).

---
### 2.1 Модель МакКаллока-Питса (M-P neuron)
Представляет собой **бинарный элемент**, который суммирует входные сигналы, умноженные на веса, и выдает 1, если сумма превышает порог, иначе 0.
$$y = \begin{cases} 1, & \text{если } \sum_{i} w_i x_i \geq \theta \\ 0, & \text{иначе} \end{cases}​$$
где:
- $x_i$ — входы (0 или 1),
- $w_i$​ — веса,
- $\theta$ — порог активации,
- $y$ — выход нейрона (0 или 1).

Модель двоичная, нелинейная, и служила для формализации принципов логических операций.

---
### 2.2 Персептрон
#### Структура персептрона
- **Входы:** Набор числовых значений $x_1, x_2, \dots, x_n$​.
- **Веса:** Для каждого входа есть вес $w_1, w_2, \dots, w_n$​, показывающий значимость соответствующего входа.
- **Смещение (bias):** $b$, дополнительный параметр, сдвигающий границу принятия решения.
- **Сумматор:** Вычисляет взвешенную сумму входов:
$$z = \sum_{i=1}^n w_i x_i + b$$
- **Функция активации:** Обычно пороговая функция, которая выдаёт:
$$y = \begin{cases} 1, & z \geq 0 \\ 0, & z < 0 \end{cases}$$
То есть персептрон принимает решение «да» или «нет», «класс 1» или «класс 0».

**Обучение** — процесс корректировки весов и смещения для правильной классификации тренировочных примеров.

**Теорема сходимости перцептрона** — это теорема. Она показывает, что элементарный перцептрон, обучаемый по методу коррекции ошибки (с квантованием или без него), независимо от начального состояния весовых коэффициентов и последовательности появления стимулов всегда приведёт к достижению решения за конечный промежуток времени

---
## Ссылки