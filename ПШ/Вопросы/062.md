---
Приоритет:
  - ГЛАВНЫЙ
Direction:
  - "[[ПШ]]"
Discipline: Системы искусственного интеллекта
tags:
  - готово
---
[[061.md|<< Предыдущий вопрос]] | [[063.md|Следующий вопрос >>]]
## Вопрос
Сигмоидальный нейрон. Функции активации нейронов.

### Что возможно нужно будет рассказать?
Вопрос сосредоточен на одной из ключевых составляющих современных искусственных нейронных сетей — **функциях активации**. Особенно важно понять, почему сигмоидальная функция стала классическим выбором, какие свойства она имеет и как её применяют в нейронных моделях.
#### **Подводные камни:**
- Отличать классическую ступенчатую функцию (модель МакКаллока-Питса, персептрон) от плавных, дифференцируемых функций активации.
- Понять роль функций активации в нелинейности модели, что позволяет сети решать сложные задачи.
- Знать свойства сигмоидальной функции: форма, диапазон значений, производная.
- Уметь объяснить, почему производная функции важна для обучения (обратное распространение ошибки).
- Знать недостатки сигмоидальной функции (например, затухание градиентов).
- Ознакомиться с альтернативными функциями активации (ReLU, tanh, и др.) — можно упомянуть для полноты.
- Желательно привести графики функций для наглядности.

---
## Ответ
### 2.1 Значение функции активации в нейроне
**Функция активации** — это математическая функция, которая преобразует взвешенную сумму входных сигналов нейрона в его выходной сигнал.
#### Почему функция активации важна?
- **Обеспечение нелинейности**  
    Если бы нейрон просто суммировал входы с весами и передавал это дальше без нелинейного преобразования, вся сеть свелась бы к простой линейной модели — то есть не могла бы решать сложные задачи, где данные неразделимы линейной границей.  
    Нелинейная функция активации даёт возможность строить сложные, многоуровневые решения.
- **Дифференцируемость**  
    Для обучения нейросети с помощью методов градиентного спуска (например, обратного распространения ошибки) необходимо, чтобы функция активации была дифференцируемой — то есть имела производную. Это позволяет вычислять градиенты ошибки по весам и корректировать их.
- **Контроль выхода нейрона**  
    Функция активации ограничивает выход нейрона в определённом диапазоне (например, [0,1] или [-1,1]), что облегчает дальнейшую обработку сигналов и стабилизирует обучение.

---
### 2.2 Сигмоидальная функция активации
Сигмоидальная функция — одна из классических нелинейных функций активации. Она принимает любое действительное число на входе и преобразует его в значение в диапазоне от 0 до 1. Формула:  $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
- Диапазон выходных значений: от 0 до 1.
- Гладкая, монотонно возрастающая функция.
#### Преимущества
- Хорошо подходит для задач бинарной классификации, где выход можно интерпретировать как вероятность принадлежности к классу.
- Производная проста и удобна для вычисления, что облегчает обучение методом обратного распространения ошибки.
#### Недостатки
- **«Исчезающий градиент»** — при больших положительных или отрицательных значениях zzz производная близка к нулю, из-за чего обучение глубоких сетей замедляется.
- Выход всегда положителен, что может замедлять сходимость сети.
- В современных архитектурах чаще применяют другие функции (ReLU, Leaky ReLU), но сигмоида всё ещё используется в некоторых задачах.

---
### 2.3 Другие функции активации (кратко)
- Гиперболический тангенс (tanh):  
    - Формула: $$\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$  
    - Выход от -1 до 1, центрирован на нуле, что помогает обучению.
	- Используется в скрытых слоях нейросетей.
- ReLU (Rectified Linear Unit):  
    - Формула:  $$\mathrm{ReLU}(x) = \max(0, x)$$
    - Быстрое вычисление, уменьшение проблемы затухания градиентов.
    - Выход 0, если $z < 0$, и равен $z$, если $z \geq 0$.

| Свойство                       | Сигмоида                         | tanh           | ReLU                            |
| ------------------------------ | -------------------------------- | -------------- | ------------------------------- |
| Диапазон выхода                | (0, 1)                           | (-1, 1)        | [0, ∞)                          |
| Центрирована вокруг 0          | Нет                              | Да             | Нет                             |
| Производная                    | Мала при больших                 | Более выражена | Постоянна (1 для >0)            |
| Проблема исчезающего градиента | Есть                             | Есть (меньше)  | Нет                             |
| Вычислительная эффективность   | Средняя                          | Средняя        | Высокая                         |
| Использование                  | Выходной слой для бинарных задач | Скрытые слои   | Скрытые слои (современные сети) |

---
### 2.4 Недостатки сигмоидальной функции
- Склонна к **исчезающему градиенту**, что замедляет обучение глубоких сетей.
- Не центрирована вокруг нуля, из-за чего градиенты могут быть менее стабильны.
- Выход всегда положителен — снижает скорость сходимости.
- На практике чаще заменяется на ReLU или tanh, особенно в скрытых слоях.

---
## Ссылки
